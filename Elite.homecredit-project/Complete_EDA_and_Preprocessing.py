# -*- coding: utf-8 -*-
"""Test&Train EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wMwoa3xy4kp8JiePaKzIvo1xu3JmMk-z
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings('ignore')
import lightgbm as lgb
import gc
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
sns.set_style('darkgrid')
# %matplotlib inline

POS_CASH_balance = pd.read_csv("/content/POS_CASH_balance.csv")
bureau=pd.read_csv("/content/bureau.csv")
bureau_balance=pd.read_csv("/content/bureau_balance.csv")
credit_card_balance = pd.read_csv("/content/credit_card_balance.csv")
installments_payments = pd.read_csv("/content/installments_payments.csv")
previous_application = pd.read_csv("/content/previous_application.csv")

print("POS CASH Balance Shape: ", POS_CASH_balance.shape)
print("Bureau Shape: ", bureau.shape)
print("Bureau Balance Shape: ", bureau_balance.shape)
print("Credit_card_balance Shape: ", credit_card_balance.shape)
print("Installments_payments Shape: ", installments_payments.shape)
print("Previous_application Shape: ", previous_application.shape)

application_train = pd.read_csv("/content/application_train.csv")
application_test = pd.read_csv("/content/application_test.csv")

print("Application Shape: ", application_train.shape)
print("Application Shape: ", application_test.shape)

def imputeCategoricalMissingValuesUsingMode(df):
  columnName = getCnamesObject(df)
  for col in columnName: df[col].fillna(df[col].mode().values[0], inplace = True)

def imputeNumericMissingValuesUsingMean(df):
  columnName = getCnamesNumeric(df)
  for col in columnName: df[col].fillna(df[col].mean(), inplace = True)

def drawPieChartForCategoricalFeatures(df, columnNames):
  number_of_rows = int((len(columnNames) + 1)/2)
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(columnNames)):
    col = columnNames[i]
    percentage = (df[col].value_counts(dropna=True, normalize=True)*100).to_list()
    labels = list(df[col].unique())
    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(labels, percentage)]
    sizes = list(df[col].value_counts(dropna=True))
    ax1 = plt.subplot(number_of_rows,2,i + 1)
    wedges, autotexts = ax1.pie(sizes, startangle=90)
    #ax1.axis('equal')

    ax1.legend(wedges, labels,
              title=col,
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))
    plt.title(col)
  plt.show()
    
def getCnamesNumeric(df):
    return list(df.select_dtypes(exclude='object').columns)

def getCnamesObject(df):
    return list(df.select_dtypes(include='object').columns)

def getDetailsAboutMissingValuesAllColumns(df):
  return df.isna().sum()/df.shape[0]

def getDetailsAboutOnlyMissingValuesColumns(df):
  print("Shape: ", df.shape)
  return df[list(df.columns[df.isna().any()])].isna().sum()/df.shape[0]

def dropColumns(df, columnNames):
  for column in columnNames:
    df = df.drop(column, axis = 1)
  return df

def plotCountPlotForCategoricalFeatures(df, cnamesObject):
  number_of_rows = (len(cnamesObject) + 1)/2
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(cnamesObject)):
    plt.subplot(number_of_rows,2,i+1)
    sns.countplot(y=cnamesObject[i], data = df)
    plt.title(cnamesObject[i])
    plt.tight_layout()

def distributionOfCategoricalFeaturesWRTTarget(df, cnamesObject):
  number_of_rows = (len(cnamesObject) + 1)/2
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(cnamesObject)):
    plt.subplot(number_of_rows,2,i+1)
    sns.countplot(data=df, hue="TARGET", y=cnamesObject[i])
    plt.title(cnamesObject[i])
    plt.tight_layout()

def drawCorrelationMatrix(df) :
  length = len(getCnamesNumeric(df))
  correlaionMatrix = df.corr()
  plt.figure(figsize=(length,length*0.8))
  sns.heatmap(correlaionMatrix, annot=True, cmap = 'viridis')

def drawDistributionPlot(df, cnamesNumeric):
  number_of_rows = (len(cnamesNumeric) + 1)/2
  plt.figure(figsize=(20, 4*number_of_rows))

  for i in range(0, len(cnamesNumeric)):
    plt.subplot(number_of_rows,3,i+1)
    sns.kdeplot(df[cnamesNumeric[i]])
    plt.title(cnamesNumeric[i])
    plt.tight_layout()

def corr_columnwise(df):
  columns = df.columns
  for col in columns:
    at_corr_col = at_corr[col]
    print(col+" +ve")
    print(at_corr_col[at_corr_col>0.9].index.to_list())
    print(col+" -ve")
    print(at_corr_col[at_corr_col<-0.9].index.to_list())

def remove_corr_columns(df,atNC,atCC,df_corr):
  threshold = 0.9
  columns = np.full((df_corr.shape[0],), True, dtype=bool)
  for i in range(df_corr.shape[0]):
      for j in range(i+1, df_corr.shape[0]):
          if df_corr.iloc[i,j] >= threshold:
              if columns[j]:
                  columns[j] = False
  x_temp=df[atNC].columns[columns].to_list()
  return pd.concat([df[x_temp],df[atCC]],axis=1)

def checkOutlier(data, cnames_numeric):
    number_of_rows = (len(cnames_numeric) + 1)/2
    plt.figure(figsize=(20, 4*number_of_rows))
    
    for i in range(0,len(cnames_numeric)):
        plt.subplot(number_of_rows,4,i+1)
        sns.boxplot(x=data[cnames_numeric[i]])
        plt.title(cnames_numeric[i])
        plt.tight_layout()

def removeOutlier(data, cnamesNumeric):
    for i in cnamesNumeric:
        q75, q25 = np.percentile(data.loc[:,i], [75 ,25])
        #Calculate IQR
        iqr = q75 - q25
        #Calculate inner and outer fence
        minimum = q25 - (iqr*1.5)
        maximum = q75 + (iqr*1.5)
        #Replace with NA
        data.loc[data.loc[:,i] < minimum,i] = np.nan
        data.loc[data.loc[:,i] > maximum,i] = np.nan
    imputeNumericMissingValuesUsingMean(data)
    return data

def drawPieChartForCategoricalFeatures(df, columnNames):
  number_of_rows = int((len(columnNames) + 1)/2)
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(columnNames)):
    col = columnNames[i]
    percentage = (df[col].value_counts(dropna=True, normalize=True)*100).to_list()
    labels = list(df[col].unique())
    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(labels, percentage)]
    sizes = list(df[col].value_counts(dropna=True))
    ax1 = plt.subplot(number_of_rows,2,i + 1)
    wedges, autotexts = ax1.pie(sizes, startangle=90)
    #ax1.axis('equal')

    ax1.legend(wedges, labels,
              title=col,
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))
    plt.title(col)
plt.show()

def applyOneHotEncoding(df, Columns):
  for col in Columns:
    ohc = OneHotEncoder()
    ohe = ohc.fit_transform(df[col].values.reshape(-1,1)).toarray()
    dfOneHot = pd.DataFrame(ohe, columns=[col+"_"+str(ohc.categories_[0][i])
                                                for i in range(len(ohc.categories_[0]))])
    df = pd.concat([df, dfOneHot.reindex(df.index)], axis=1)
    df = df.drop(col, axis = 1)
  return df
  
def plot_feature_importances(df):
    
    df = df.sort_values('importance', ascending = False).reset_index()
    df['importance_normalized'] = df['importance'] / df['importance'].sum()
    plt.figure(figsize = (10, 6))
    ax = plt.subplot()
    ax.barh(list(reversed(list(df.index[:10]))), df['importance_normalized'].head(10))

    ax.set_yticks(list(reversed(list(df.index[:10]))))
    ax.set_yticklabels(df['feature'].head(10))
  
    plt.xlabel('Percentage'); plt.title('Feature Importances')
    plt.show()
    
    return df

POS_CASH_balance.info()

POS_CASH_balance.describe()

POS_CASH_balance.head()

POS_CASH_balance.nunique()

POS_CASH_balance.duplicated()

POS_CASH_balance.isnull().sum()

# checking missing data
total = POS_CASH_balance.isnull().sum().sort_values(ascending = False)
percent = (POS_CASH_balance.isnull().sum()/POS_CASH_balance.isnull().count()*100).sort_values(ascending = False)
missing_bureaueau_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_bureaueau_data

paCC = getCnamesObject(POS_CASH_balance)
print("Number of categorical columns in previous_application: ", len(paCC), "\n", paCC)
paCC.remove('SK_ID_PREV')
paCC.remove('SK_ID_CURR')

paNC = getCnamesNumeric(POS_CASH_balance)
print("Number of numerical columns in previous_application: ", len(paNC), "\n", paNC)

plotCountPlotForCategoricalFeatures(POS_CASH_balance, paCC)

POS_CASH_balance['NAME_CONTRACT_STATUS'].value_counts()

drawDistributionPlot(POS_CASH_balance, paNC)

drawCorrelationMatrix(POS_CASH_balance)

getDetailsAboutOnlyMissingValuesColumns(POS_CASH_balance)

imputeNumericMissingValuesUsingMean(POS_CASH_balance)

getDetailsAboutOnlyMissingValuesColumns(POS_CASH_balance)

bureau.info()

bureau.describe()

bureau.head()

bureau.nunique()

buCC = getCnamesObject(bureau)
print("Number of categorical columns in Bureau: ", len(buCC), "\n", buCC)
buCC.remove('SK_ID_BUREAU')
buCC.remove('SK_ID_CURR')

buNC = getCnamesNumeric(bureau)
print("Number of numerical columns in Bureau: ", len(buNC), "\n", buNC)

# checking missing data percentage
total = bureau.isnull().sum().sort_values(ascending = False)
percent = (bureau.isnull().sum()/bureau.isnull().count()*100).sort_values(ascending = False)
missing_bureau_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_bureau_data

bureau.isnull().sum()

plotCountPlotForCategoricalFeatures(bureau,buCC)

bureau[['CREDIT_ACTIVE']].value_counts()

bureau[['CREDIT_CURRENCY']].value_counts()

bureau[['CREDIT_TYPE']].value_counts()

drawDistributionPlot(bureau, buNC)

drawCorrelationMatrix(bureau)

getDetailsAboutOnlyMissingValuesColumns(bureau)

imputeNumericMissingValuesUsingMean(bureau)

getDetailsAboutOnlyMissingValuesColumns(bureau)

bureau_balance.info()

bureau_balance.describe()

bureau_balance.head()

bureau_balance.nunique()

bureau_balance.isnull().sum()

# checking missing data
total = bureau_balance.isnull().sum().sort_values(ascending = False)
percent = (bureau_balance.isnull().sum()/bureau_balance.isnull().count()*100).sort_values(ascending = False)
missing_bureau_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_bureau_data

balCC = getCnamesObject(bureau_balance)
print("Number of categorical columns in balance bureau: ", len(balCC), "\n", balCC)
balCC.remove('SK_ID_BUREAU')

balNC = getCnamesNumeric(bureau_balance)
print("Number of numerical columns in balance bureau: ", len(balNC), "\n", balNC)

plotCountPlotForCategoricalFeatures(bureau_balance,balCC)

bureau_balance[['STATUS']].value_counts()

drawDistributionPlot(bureau_balance,balNC)

drawCorrelationMatrix(bureau_balance)

getDetailsAboutOnlyMissingValuesColumns(bureau_balance)

aTrainCC = getCnamesObject(application_train)
print("Number of categorical columns in previous_application: ", len(aTrainCC), "\n", aTrainCC)
aTrainCC.remove('SK_ID_CURR')

paCC = getCnamesObject(previous_application)
print("Number of categorical columns in previous_application: ", len(paCC), "\n", paCC)
paCC.remove('SK_ID_PREV')
paCC.remove('SK_ID_CURR')

ipCC = getCnamesObject(installments_payments)
print("Number of categorical columns in installments_payments: ", len(ipCC), "\n", ipCC)

ccbCC = getCnamesObject(credit_card_balance)
print("Number of categorical columns in credit_card_balance: ", len(ccbCC), "\n", ccbCC)
ccbCC.remove('SK_ID_PREV')
ccbCC.remove('SK_ID_CURR')

paNC = getCnamesNumeric(previous_application)
print("Number of numerical columns in previous_application: ", len(paNC), "\n", paNC)
ipNC = getCnamesNumeric(installments_payments)
print("Number of numerical columns in installments_payments: ", len(ipNC), "\n", ipNC)
ccbNC = getCnamesNumeric(credit_card_balance)
print("Number of numerical columns in credit_card_balance: ", len(ccbNC), "\n", ccbNC)

plotCountPlotForCategoricalFeatures(previous_application, paCC)

plotCountPlotForCategoricalFeatures(credit_card_balance, ccbCC)

distributionOfCategoricalFeaturesWRTTarget(application_train, aTrainCC)

drawPieChartForCategoricalFeatures(credit_card_balance, ccbCC)

targetTemp = ['TARGET']
drawPieChartForCategoricalFeatures(application_train, targetTemp)

drawPieChartForCategoricalFeatures(previous_application, paCC)

drawDistributionPlot(previous_application, paNC)

drawDistributionPlot(installments_payments, ipNC)

drawDistributionPlot(credit_card_balance, ccbNC)

drawCorrelationMatrix(previous_application)

drawCorrelationMatrix(installments_payments)

drawCorrelationMatrix(credit_card_balance)

getDetailsAboutOnlyMissingValuesColumns(previous_application)

#Drop the data with more than 50% missing values becuase imputing them doesn't makes sense
missingValueColumns = ['AMT_DOWN_PAYMENT', 'RATE_DOWN_PAYMENT','RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED','NAME_TYPE_SUITE']
previous_application = dropColumns(previous_application, missingValueColumns)

getDetailsAboutOnlyMissingValuesColumns(installments_payments)

getDetailsAboutOnlyMissingValuesColumns(credit_card_balance)

imputeNumericMissingValuesUsingMean(previous_application)
imputeNumericMissingValuesUsingMean(installments_payments)
imputeNumericMissingValuesUsingMean(credit_card_balance)

imputeCategoricalMissingValuesUsingMode(previous_application)

application_train.columns.to_list()

pd.set_option('max_columns', None)
application_train.head()

pd.reset_option('max_columns')

application_train.describe()



atCC = getCnamesObject(application_train)
print("Number of categorical columns in application_train: ", len(atCC), "\n", atCC)

atNC = getCnamesNumeric(application_train)
print("Number of numerical columns in application_train: ", len(atNC), "\n", atNC)

atestCC = getCnamesObject(application_test)
print("Number of categorical columns in application_test: ", len(atestCC), "\n", atestCC)

atestNC = getCnamesNumeric(application_test)
print("Number of numerical columns in application_test: ", len(atestNC), "\n", atestNC)

atCC.remove('SK_ID_CURR')
atestCC.remove('SK_ID_CURR')

application_train[atCC]

plotCountPlotForCategoricalFeatures(application_train, atCC)

distributionOfCategoricalFeaturesWRTTarget(application_train, atCC)

drawDistributionPlot(application_train, atNC)

drawCorrelationMatrix(application_train)

pd.set_option('max_columns', None)
pd.set_option('max_rows', None)

at_corr=application_train.corr()

corr_columnwise(application_train.corr())

pd.reset_option('max_columns')
pd.reset_option('max_rows')

new_application_train = remove_corr_columns(application_train,atNC,atCC,at_corr)

labels = new_application_train['TARGET']

new_application_train, new_application_test = new_application_train.align(application_test, join = 'inner', axis = 1)

new_application_train['TARGET'] = labels

print('Training Features shape: ', new_application_train.shape)
print('Testing Features shape: ', new_application_test.shape)

print(getDetailsAboutOnlyMissingValuesColumns(new_application_train).sort_values(ascending=False))
print(getDetailsAboutOnlyMissingValuesColumns(new_application_test).sort_values(ascending=False))

imputeCategoricalMissingValuesUsingMode(new_application_train)
imputeNumericMissingValuesUsingMean(new_application_train)

imputeCategoricalMissingValuesUsingMode(new_application_test)
imputeNumericMissingValuesUsingMean(new_application_test)

checkOutlier(new_application_train, getCnamesNumeric(new_application_train))

#Extract out target variable
dfTarget = new_application_train.TARGET
new_application_train = new_application_train.drop('TARGET', axis = 1)

atNC = getCnamesNumeric(new_application_train)
atCC = getCnamesObject(new_application_train)

new_application_train = removeOutlier(new_application_train, atNC)

"""#######################Exploring Categorical Variable########################"""

new_application_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)

atCCTemp = atCC
drawPieChartForCategoricalFeatures(new_application_train, atCCTemp)

drawPieChartForCategoricalFeatures(new_application_train, ['OCCUPATION_TYPE','ORGANIZATION_TYPE'])

dfTarget.value_counts(dropna=True, normalize=True)*100

new_application_train_Temp = applyOneHotEncoding(new_application_train, atCCTemp)
new_application_test_Temp = applyOneHotEncoding(new_application_test, atCCTemp)

new_application_train_Temp = new_application_train_Temp.drop(['OCCUPATION_TYPE','ORGANIZATION_TYPE'], axis = 1)

new_application_train_Temp = remove_corr_columns(new_application_train_Temp,
                                                  getCnamesNumeric(new_application_train_Temp),
                                                  getCnamesObject(new_application_train_Temp),
                                                  new_application_train_Temp.corr())

new_application_train_Temp, new_application_test_Temp = new_application_train_Temp.align(new_application_test_Temp, join = 'inner', axis = 1)

print('Training Features shape: ', new_application_train_Temp.shape)
print('Testing Features shape: ', new_application_test_Temp.shape)

train = new_application_train_Temp.copy()
test = new_application_test_Temp.copy()
train_labels = labels.copy()
features = list(train.columns)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(train)
train = sc.transform(train)
test = sc.transform(test)

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)

random_forest.fit(train, train_labels)

feature_importance_values = random_forest.feature_importances_
feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})

predictions = random_forest.predict_proba(test)[:, 1]

feature_importances.sort_values('importance', ascending = False)[:10]

feature_importances_sorted = plot_feature_importances(feature_importances)

train = new_application_train_Temp.copy()
test = new_application_test_Temp.copy()

train_ids = application_train['SK_ID_CURR']
test_ids = application_test['SK_ID_CURR']

labels = labels.copy()
feature_names = list(train.columns)

print('Training Data Shape: ', train.shape)
print('Testing Data Shape: ', test.shape)

def model(features, test_features, encoding = 'ohe', n_folds = 5):

    features = np.array(features)
    test_features = np.array(test_features)
    
    # Create the kfold object
    k_fold = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 50)
    
    # Feature importances
    feature_importance_values = np.zeros(len(feature_names))
    
    # Test predictions
    test_predictions = np.zeros(test_features.shape[0])
    
    # Empty array for out of fold validation predictions
    out_of_fold = np.zeros(features.shape[0])
    
    # Lists for recording validation and training scores
    valid_scores = []
    train_scores = []
    
    # Iterate through each fold
    for train_indices, valid_indices in k_fold.split(features, labels):
        
        # Training data for the fold
        train_features, train_labels = features[train_indices], labels[train_indices]
        # Validation data for the fold
        valid_features, valid_labels = features[valid_indices], labels[valid_indices]
        
        # Create the model
        model = lgb.LGBMClassifier(n_estimators=10000,
        learning_rate=0.03,
        num_leaves=34,
        colsample_bytree=0.9,
        subsample=0.8,
        max_depth=8,
        reg_alpha=.1,
        reg_lambda=.1,
        min_split_gain=.01,
        min_child_weight=250,
        objective = 'binary')
        
        # Train the model
        model.fit(train_features, train_labels, eval_metric = 'auc',
                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'], early_stopping_rounds = 200, verbose = 200)
        
        # Best iteration
        best_iteration = model.best_iteration_
        
        # Feature importances
        feature_importance_values += model.feature_importances_ / k_fold.n_splits
        
        # Make predictions
        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits
        
        # Record the out of fold predictions
        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]
        
        # Record the best score
        valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']
        
        valid_scores.append(valid_score)
        train_scores.append(train_score)
        
        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()
        
    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})
    
    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})
    
    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)
    
    # Add the overall scores to the metrics
    valid_scores.append(valid_auc)
    train_scores.append(np.mean(train_scores))
    
    # Needed for creating dataframe of validation scores
    fold_names = list(range(n_folds))
    fold_names.append('overall')
    
    # Dataframe of validation scores
    metrics = pd.DataFrame({'fold': fold_names,
                            'train': train_scores,
                            'valid': valid_scores}) 
    
    return submission, feature_importances, metrics

submission, feature_importance, metrics = model(train, test)

metrics

submission.head()

submission.to_csv('Elite17.csv', index = False)

gc.collect()