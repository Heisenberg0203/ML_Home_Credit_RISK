# -*- coding: utf-8 -*-
"""Elite_Model_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16pj_nq2PA4nqjVF_dNA1uZ9HhwxZ7y9Q
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
import warnings
warnings.filterwarnings('ignore')
import lightgbm as lgb
import gc
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelEncoder
sns.set_style('darkgrid')
# %matplotlib inline

def reduce_mem_usage(df):
   
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    
    return df

#CONVERT AND RETURN NEW FILE

def import_data(file):
    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)
    df = reduce_mem_usage(df)
    return df

!ls /content/

application_train = import_data("/content/application_train.csv")

application_test=import_data("/content/application_test.csv")

bur_bal = import_data("/content/bureau_balance.csv")

bur = import_data("/content/bureau.csv")

credit_bal = import_data("/content/credit_card_balance.csv")

installment_payments = import_data("/content/installments_payments.csv")

pos_bal = import_data("/content/POS_CASH_balance.csv")

prev_application = import_data("/content/previous_application.csv")

gc.enable()
gc.collect()

application_train_org = application_train.copy()

print("Application Shape: ", application_train.shape)
print("Application Shape: ", application_test.shape)

def getCnamesNumeric(df):
    return list(df.select_dtypes(exclude=['category','object']).columns)

def getCnamesObject(df):
    return list(df.select_dtypes(include=['category','object']).columns)

def getDetailsAboutMissingValuesAllColumns(df):
  return df.isna().sum()/df.shape[0]

def getDetailsAboutOnlyMissingValuesColumns(df):
  print("Shape: ", df.shape)
  return df[list(df.columns[df.isna().any()])].isna().sum()/df.shape[0]

def dropColumns(df, columnNames):
  for column in columnNames:
    df = df.drop(column, axis = 1)
  return df

def imputeCategoricalMissingValuesUsingMode(df):
  columnName = getCnamesObject(df)
  for col in columnName: df[col].fillna(df[col].mode().values[0], inplace = True)

def imputeNumericMissingValuesUsingMean(df):
  columnName = getCnamesNumeric(df)
  for col in columnName: df[col].fillna(df[col].mean(), inplace = True)

def imputeNumericMissingValuesUsingMode(df):
  columnName = getCnamesNumeric(df)
  for col in columnName: df[col].fillna(df[col].mode()[0], inplace = True)
def imputeNumericMissingValuesUsingMedian(df):
  columnName = getCnamesNumeric(df)
  for col in columnName: df[col].fillna(df[col].median(), inplace = True)



def plotCountPlotForCategoricalFeatures(df, cnamesObject):
  number_of_rows = (len(cnamesObject) + 1)/2
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(cnamesObject)):
    plt.subplot(number_of_rows,2,i+1)
    sns.countplot(y=cnamesObject[i], data = df)
    plt.title(cnamesObject[i])
    plt.tight_layout()

def distributionOfCategoricalFeaturesWRTTarget(df, cnamesObject):
  number_of_rows = (len(cnamesObject) + 1)/2
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(cnamesObject)):
    plt.subplot(number_of_rows,2,i+1)
    sns.countplot(data=df, hue="TARGET", y=cnamesObject[i])
    plt.title(cnamesObject[i])
    plt.tight_layout()

def drawCorrelationMatrix(df) :
  length = len(getCnamesNumeric(df))
  correlaionMatrix = df.corr()
  plt.figure(figsize=(length,length*0.8))
  sns.heatmap(correlaionMatrix, annot=True, cmap = 'viridis')

def drawDistributionPlot(df, cnamesNumeric):
  number_of_rows = (len(cnamesNumeric) + 1)/2
  plt.figure(figsize=(20, 4*number_of_rows))

  for i in range(0, len(cnamesNumeric)):
    plt.subplot(number_of_rows,3,i+1)
    sns.kdeplot(df[cnamesNumeric[i]])
    plt.title(cnamesNumeric[i])
    plt.tight_layout()

def corr_columnwise(df):
  columns = df.columns
  for col in columns:
    at_corr_col = at_corr[col]
    print(col+" +ve")
    print(at_corr_col[at_corr_col>0.9].index.to_list())
    print(col+" -ve")
    print(at_corr_col[at_corr_col<-0.9].index.to_list())

def remove_corr_columns(df,atNC,atCC,df_corr):
  threshold = 0.9
  columns = np.full((df_corr.shape[0],), True, dtype=bool)
  for i in range(df_corr.shape[0]):
      for j in range(i+1, df_corr.shape[0]):
          if df_corr.iloc[i,j] >= threshold:
              if columns[j]:
                  columns[j] = False
  x_temp=df[atNC].columns[columns].to_list()
  return pd.concat([df[x_temp],df[atCC]],axis=1)

def checkOutlier(data, cnames_numeric):
    number_of_rows = (len(cnames_numeric) + 1)/2
    plt.figure(figsize=(20, 4*number_of_rows))
    
    for i in range(0,len(cnames_numeric)):
        plt.subplot(number_of_rows,4,i+1)
        sns.boxplot(x=data[cnames_numeric[i]])
        plt.title(cnames_numeric[i])
        plt.tight_layout()

def removeOutlier(data, cnamesNumeric):
    for i in cnamesNumeric:
        q75, q25 = np.percentile(data.loc[:,i], [75 ,25])
        #Calculate IQR
        iqr = q75 - q25
        #Calculate inner and outer fence
        minimum = q25 - (iqr*1.5)
        maximum = q75 + (iqr*1.5)
        #Replace with NA
        data.loc[data.loc[:,i] < minimum,i] = np.nan
        data.loc[data.loc[:,i] > maximum,i] = np.nan
    imputeNumericMissingValuesUsingMean(data)
    return data

def drawPieChartForCategoricalFeatures(df, columnNames):
  number_of_rows = int((len(columnNames) + 1)/2)
  plt.figure(figsize=(20, 6*number_of_rows))

  for i in range(0,len(columnNames)):
    col = columnNames[i]
    percentage = (df[col].value_counts(dropna=True, normalize=True)*100).to_list()
    labels = list(df[col].unique())
    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(labels, percentage)]
    sizes = list(df[col].value_counts(dropna=True))
    ax1 = plt.subplot(number_of_rows,2,i + 1)
    wedges, autotexts = ax1.pie(sizes, startangle=90)
    #ax1.axis('equal')

    ax1.legend(wedges, labels,
              title=col,
              loc="center left",
              bbox_to_anchor=(1, 0, 0.5, 1))
    plt.title(col)
plt.show()

def applyOneHotEncoding(df, Columns):
  for col in Columns:
    ohc = OneHotEncoder()
    ohe = ohc.fit_transform(df[col].values.reshape(-1,1)).toarray()
    dfOneHot = pd.DataFrame(ohe, columns=[col+"_"+str(ohc.categories_[0][i])
                                                for i in range(len(ohc.categories_[0]))])
    df = pd.concat([df, dfOneHot.reindex(df.index)], axis=1)
    df = df.drop(col, axis = 1)
  return df
  
def plot_feature_importances(df):
    
    df = df.sort_values('importance', ascending = False).reset_index()
    df['importance_normalized'] = df['importance'] / df['importance'].sum()
    plt.figure(figsize = (10, 6))
    ax = plt.subplot()
    ax.barh(list(reversed(list(df.index[:10]))), df['importance_normalized'].head(10))

    ax.set_yticks(list(reversed(list(df.index[:10]))))
    ax.set_yticklabels(df['feature'].head(10))
  
    plt.xlabel('Percentage'); plt.title('Feature Importances')
    plt.show()
    
    return df

def makereadyforjoin(df):
  temp = df['SK_ID_CURR']
  df=df[getCnamesNumeric(df)]
  df['SK_ID_CURR']=temp
  df=df.groupby('SK_ID_CURR').agg(['mean','max','sum','var','median'])
  df.reset_index(inplace=True)
  df.columns = pd.Index([e[0] + "_" + e[1].upper() for e in df.columns.tolist()])
  df.rename(columns={'SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)
  return df

prev_application = makereadyforjoin(prev_application)

credit_bal = makereadyforjoin(credit_bal)

pos_bal = makereadyforjoin(pos_bal)

installment_payments = makereadyforjoin(installment_payments)

application_train = application_train.merge(right=prev_application, how='left', on='SK_ID_CURR')
application_test = application_test.merge(right=prev_application, how='left', on='SK_ID_CURR')

application_train = application_train.merge(right=credit_bal, how='left', on='SK_ID_CURR')
application_test = application_test.merge(right=credit_bal, how='left', on='SK_ID_CURR')

application_train = application_train.merge(right=pos_bal, how='left', on='SK_ID_CURR')
application_test = application_test.merge(right=pos_bal, how='left', on='SK_ID_CURR')

application_train = application_train.merge(right=installment_payments, how='left', on='SK_ID_CURR')
application_test = application_test.merge(right=installment_payments, how='left', on='SK_ID_CURR')

agg = {'MONTHS_BALANCE': ['min', 'max','size','mean','var','median']}
bur_bal = bur_bal.groupby('SK_ID_BUREAU').agg(agg)
bur_bal.columns = pd.Index([e[0] + "_" + e[1].upper() for e in bur_bal.columns.tolist()])
bur_bal = bur.join(bur_bal, how='left', on='SK_ID_BUREAU')
bur_bal.drop(columns= 'SK_ID_BUREAU', inplace= True) 
temp = bur_bal['SK_ID_CURR']
bur_bal = bur_bal[getCnamesNumeric(bur_bal)]
bur_bal['SK_ID_CURR']=temp 
bur_bal = bur_bal.groupby('SK_ID_CURR').agg(['min', 'max','size','mean','var','median'])
bur_bal.reset_index(inplace=True)
bur_bal.columns = pd.Index(['BUR_' + e[0] + "_" + e[1].upper() for e in bur_bal.columns.tolist()])
bur_bal.head() 
bur_bal['SK_ID_CURR']=bur_bal['BUR_SK_ID_CURR_']
bur_bal.drop('BUR_SK_ID_CURR_',axis=1,inplace=True) 
application_train = application_train.merge(right=bur_bal, how='left', on='SK_ID_CURR')
application_test = application_test.merge(right=bur_bal, how='left', on='SK_ID_CURR')

del application_train['SK_ID_CURR']
del application_test['SK_ID_CURR']
del credit_bal
del pos_bal
del installment_payments
del prev_application
gc.enable()
gc.collect()

application_train.head()

application_train['INCOME_CREDIT_PERC'] = application_train['AMT_INCOME_TOTAL'] / application_train['AMT_CREDIT']
application_train['INCOME_PER_PERSON'] = application_train['AMT_INCOME_TOTAL'] / application_train['CNT_FAM_MEMBERS']
application_train['ANNUITY_INCOME_PERC'] = application_train['AMT_ANNUITY'] / application_train['AMT_INCOME_TOTAL']
application_train['LOAN_INCOME_RATIO'] = application_train['AMT_CREDIT'] / application_train['AMT_INCOME_TOTAL']
application_train['CONSUMER_GOODS_RATIO'] = application_train['AMT_CREDIT'] / application_train['AMT_GOODS_PRICE']
application_train['ANNUITY LENGTH'] = application_train['AMT_CREDIT'] / application_train['AMT_ANNUITY']

application_test['INCOME_CREDIT_PERC'] = application_test['AMT_INCOME_TOTAL'] / application_test['AMT_CREDIT']
application_test['INCOME_PER_PERSON'] = application_test['AMT_INCOME_TOTAL'] / application_test['CNT_FAM_MEMBERS']
application_test['ANNUITY_INCOME_PERC'] = application_test['AMT_ANNUITY'] / application_test['AMT_INCOME_TOTAL']
application_test['LOAN_INCOME_RATIO'] = application_test['AMT_CREDIT'] / application_test['AMT_INCOME_TOTAL']
application_test['CONSUMER_GOODS_RATIO'] = application_test['AMT_CREDIT'] / application_test['AMT_GOODS_PRICE']
application_test['ANNUITY LENGTH'] = application_test['AMT_CREDIT'] / application_test['AMT_ANNUITY']

application_train.describe()

application_train=application_train.replace(np.inf, np.nan)
application_train=application_train.replace(-np.inf, np.nan)

application_test=application_test.replace(np.inf, np.nan)
application_test=application_test.replace(-np.inf, np.nan)

application_train_final = application_train.copy()
application_test_final = application_test.copy()

application_train = application_train_final.copy()
application_test = application_test_final.copy()

atCC = getCnamesObject(application_train)
print("Number of categorical columns in application_train: ", len(atCC), "\n", atCC)

atNC = getCnamesNumeric(application_train)
print("Number of numerical columns in application_train: ", len(atNC), "\n", atNC)

atestCC = getCnamesObject(application_test)
print("Number of categorical columns in application_test: ", len(atestCC), "\n", atestCC)

atestNC = getCnamesNumeric(application_test)
print("Number of numerical columns in application_test: ", len(atestNC), "\n", atestNC)

# atCC.remove('SK_ID_CURR')
# atestCC.remove('SK_ID_CURR')

at_corr=application_train.corr()

new_application_train_all_features = application_train.copy()
new_application_train = remove_corr_columns(application_train,atNC,atCC,at_corr)

labels = new_application_train['TARGET']

new_application_train, new_application_test = new_application_train.align(application_test, join = 'inner', axis = 1)
new_application_train_all_features, new_application_test_all_features = new_application_train_all_features.align(application_test, join = 'inner', axis = 1)

new_application_train['TARGET'] = labels
new_application_train_all_features['TARGET'] = labels

print('Training Features shape: ', new_application_train.shape)
print('Training (all) Features shape: ', new_application_train_all_features.shape)
print('Testing Features shape: ', new_application_test.shape)

# print(getDetailsAboutOnlyMissingValuesColumns(new_application_train).sort_values(ascending=False))
# print(getDetailsAboutOnlyMissingValuesColumns(new_application_test).sort_values(ascending=False))

imputeCategoricalMissingValuesUsingMode(new_application_train)
imputeNumericMissingValuesUsingMedian(new_application_train)

imputeCategoricalMissingValuesUsingMode(new_application_test)
imputeNumericMissingValuesUsingMedian(new_application_test)

imputeCategoricalMissingValuesUsingMode(new_application_train_all_features)
imputeNumericMissingValuesUsingMedian(new_application_train_all_features)

imputeCategoricalMissingValuesUsingMode(new_application_test_all_features)
imputeNumericMissingValuesUsingMedian(new_application_test_all_features)

# checkOutlier(new_application_train, atNC)

#Extract out target variable
dfTarget = new_application_train.TARGET
new_application_train = new_application_train.drop('TARGET', axis = 1)

new_application_train_all_features = new_application_train_all_features.drop('TARGET', axis = 1)

atNC = getCnamesNumeric(new_application_train)
atCC = getCnamesObject(new_application_train)

atANC = getCnamesNumeric(new_application_train_all_features)
atACC = getCnamesObject(new_application_train_all_features)

# new_application_train = removeOutlier(new_application_train, atNC)

"""#######################Exploring Categorical Variable########################"""

new_application_train.select_dtypes(['object','category']).apply(pd.Series.nunique, axis = 0)

frequency_encoded_variables = ['OCCUPATION_TYPE','ORGANIZATION_TYPE']

def frequency_encoding(variable, train, test):
    t = pd.concat([train[variable], test[variable]]).value_counts().reset_index()
    t = t.reset_index()
    t.loc[t[variable] == 1, 'level_0'] = np.nan
    t.set_index('index', inplace=True)
    max_label = t['level_0'].max() + 1
    t.fillna(max_label, inplace=True)
    return t.to_dict()['level_0']

from tqdm import tqdm

for variable in tqdm(frequency_encoded_variables):
    freq_enc_dict = frequency_encoding(variable, new_application_train, new_application_test)
    new_application_train[variable] = new_application_train[variable].map(lambda x: freq_enc_dict.get(x, np.nan))
    new_application_test[variable] = new_application_test[variable].map(lambda x: freq_enc_dict.get(x, np.nan))

for variable in tqdm(frequency_encoded_variables):
    freq_enc_dict = frequency_encoding(variable, new_application_train_all_features, new_application_test_all_features)
    new_application_train_all_features[variable] = new_application_train_all_features[variable].map(lambda x: freq_enc_dict.get(x, np.nan))
    new_application_test_all_features[variable] = new_application_test_all_features[variable].map(lambda x: freq_enc_dict.get(x, np.nan))

for col in frequency_encoded_variables:
  new_application_train[col] = new_application_train[col].astype('int32')
  new_application_test[col] = new_application_test[col].astype('int32')

  new_application_train_all_features[col] = new_application_train_all_features[col].astype('int32')
  new_application_test_all_features[col] = new_application_test_all_features[col].astype('int32')

atNC = getCnamesNumeric(new_application_train)
atCC = getCnamesObject(new_application_train)

atANC = getCnamesNumeric(new_application_train_all_features)
atACC = getCnamesObject(new_application_train_all_features)

new_application_train=pd.get_dummies(new_application_train)
new_application_test = pd.get_dummies(new_application_test)

new_application_train_all_features=pd.get_dummies(new_application_train_all_features)
new_application_test_all_features = pd.get_dummies(new_application_test_all_features)

new_application_train, new_application_test = new_application_train.align(new_application_test, join = 'inner', axis = 1)

print('Training Features shape: ', new_application_train.shape)
print('Testing Features shape: ', new_application_test.shape)

new_application_train_all_features, new_application_test_all_features = new_application_train_all_features.align(new_application_test_all_features, join = 'inner', axis = 1)

print('Training Features (all) shape: ', new_application_train_all_features.shape)
print('Testing Features (all) shape: ', new_application_test_all_features.shape)

train = new_application_train.copy()
test = new_application_test.copy()
train_labels = labels.copy()
features = list(train.columns)

trainAllFt = new_application_train_all_features.copy()
testAllFt = new_application_test_all_features.copy()
train_labelsAllFt = labels.copy()
features = list(trainAllFt.columns)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(train)
train = sc.transform(train)
test = sc.transform(test)

sc1 = StandardScaler()
sc1.fit(trainAllFt)
trainAllFt = sc1.transform(trainAllFt)
testAllFt = sc1.transform(testAllFt)

train = pd.DataFrame(train,columns=new_application_train.columns)
test =pd.DataFrame(test,columns=new_application_test.columns)

trainAllFt = pd.DataFrame(trainAllFt,columns=new_application_train_all_features.columns)
testAllFt =pd.DataFrame(testAllFt,columns=new_application_test_all_features.columns)

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import StratifiedKFold

labels[labels==0].shape,labels[labels==1].shape

def model(features, test_features, modelType, encoding = 'ohe', n_folds = 5):
    
    
    # Extract the ids
    train_ids = features['SK_ID_CURR']
    test_ids = test_features['SK_ID_CURR']
    
    # Extract the labels for training
    labels = features['TARGET']
    
    # Remove the ids and target
    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])
    test_features = test_features.drop(columns = ['SK_ID_CURR'])
    
    
    print('Training Data Shape: ', features.shape)
    print('Testing Data Shape: ', test_features.shape)
    
    # Extract feature names
    feature_names = list(features.columns)
    
    # Convert to np arrays
    features = np.array(features)
    test_features = np.array(test_features)
    
    # Create the kfold object
    k_fold = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 50)
    
    # Empty array for feature importances
    feature_importance_values = np.zeros(len(feature_names))
    
    # Empty array for test predictions
    test_predictions = np.zeros(test_features.shape[0])
    
    # Empty array for out of fold validation predictions
    out_of_fold = np.zeros(features.shape[0])
    
    # Lists for recording validation and training scores
    valid_scores = []
    train_scores = []
    
    # Iterate through each fold
    for train_indices, valid_indices in k_fold.split(features,labels):
        
        # Training data for the fold
        train_features, train_labels = features[train_indices], labels[train_indices]
        # Validation data for the fold
        valid_features, valid_labels = features[valid_indices], labels[valid_indices]
        
        # Create the model
        # model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary',
        #                            class_weight = 'balanced', learning_rate = 0.03,scale_pos_weight=9,
        #                            reg_alpha = 0.01, reg_lambda = 0.01, n_jobs = -1, random_state = 50)
        model = lgb.LGBMClassifier();
        if(modelType == 1): 
          model = lgb.LGBMClassifier(
                n_jobs = -1,
                n_estimators=10000,
                learning_rate=0.02,
                num_leaves=34,
                colsample_bytree=0.9497036,
                subsample=0.8715623,
                max_depth=8,
                reg_alpha=0.041545473,
                reg_lambda=0.0735294,
                min_split_gain=0.0222415,
                min_child_weight=39.3259775,
                silent=-1,
                verbose=-1, )
        
        else: 
            model = lgb.LGBMClassifier(
                n_estimators=10000,
                learning_rate=0.03,
                num_leaves=1023,
                colsample_bytree=0.9, 
                subsample=0.8,max_depth=-1,reg_alpha=.01,reg_lambda=.01,min_child_weight=500, 
                class_weight='balanced',) 
        

        # Train the model
        model.fit(train_features, train_labels, eval_metric = 'auc',
                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],
                  eval_names = ['valid', 'train'],
                  early_stopping_rounds = 200, verbose = 200)
        
        # Record the best iteration
        best_iteration = model.best_iteration_
        
        # Record the feature importances
        feature_importance_values += model.feature_importances_ / k_fold.n_splits
        
        # Make predictions
        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits
        
        # Record the out of fold predictions
        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]
        
        # Record the best score
        valid_score = model.best_score_['valid']['auc']
        train_score = model.best_score_['train']['auc']
        
        valid_scores.append(valid_score)
        train_scores.append(train_score)
        
        # Clean up memory
        gc.enable()
        del model, train_features, valid_features
        gc.collect()
        
    # Make the submission dataframe
    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})
    
    # Make the feature importance dataframe
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})
    
    # Overall validation score
    valid_auc = roc_auc_score(labels, out_of_fold)
    
    # Add the overall scores to the metrics
    valid_scores.append(valid_auc)
    train_scores.append(np.mean(train_scores))
    
    # Needed for creating dataframe of validation scores
    fold_names = list(range(n_folds))
    fold_names.append('overall')
    
    # Dataframe of validation scores
    metrics = pd.DataFrame({'fold': fold_names,
                            'train': train_scores,
                            'valid': valid_scores}) 
    
    return submission, feature_importances, metrics

train['TARGET'] = pd.Series(labels)
train['SK_ID_CURR'] = import_data('/content/application_train.csv').SK_ID_CURR
test['SK_ID_CURR'] = import_data('/content/application_test.csv').SK_ID_CURR

trainAllFt['TARGET'] = pd.Series(labels)
trainAllFt['SK_ID_CURR'] = import_data('/content/application_train.csv').SK_ID_CURR
testAllFt['SK_ID_CURR'] = import_data('/content/application_test.csv').SK_ID_CURR

"""Applying Model on Dataset without correlated columns. (----M1----)"""

submission, feature_importances, metrics = model(train, test, 1)
print(metrics)

submission.to_csv('m1.csv', index = False)

"""------------------------M1 Done---------------------

Applying Model on Dataset all columns. (----M2----)
"""

submission, feature_importances2, metrics = model(trainAllFt, testAllFt, 1)
print(metrics)

submission.to_csv('m2.csv', index = False)

"""------------------------M2 Done---------------------

Applying Different Model on Dataset without correlated columns. (----M3----)
"""

submission, feature_importances, metrics = model(train, test, 2)
print(metrics)

submission.to_csv('m3.csv', index = False)

"""------------------------M3 Done---------------------"""

m1 = pd.read_csv('m1.csv')
m2 = pd.read_csv('m2.csv')
m3 = pd.read_csv('m3.csv')

submission = pd.DataFrame()
submission['SK_ID_CURR'] = m1['SK_ID_CURR']

submission['TARGET'] = 0.2 * m1['TARGET'] + 0.7 * m2['TARGET'] + 0.1 * m3['TARGET']

submission.to_csv('Elite_Model_2.csv', index = False)

feature_importances2.sort_values(by=['importance'], ascending=False).head(10).set_index('feature').plot(kind='bar')